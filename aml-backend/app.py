import os
import json
from flask import Flask, request, jsonify, send_file, make_response, Blueprint
from flask_cors import CORS
from werkzeug.utils import secure_filename
from datetime import datetime, timedelta
import hashlib
import shutil
import random  # –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
import threading # <-- 1. –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º threading
import sys
import glob

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from aml_integration_system import run_full_analysis # <-- 2. –°—Ç–∞—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
from unified_aml_pipeline import UnifiedAMLPipeline  # <-- 3. –ù–æ–≤—ã–π –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π pipeline
from aml_json_loader import AMLJSONDataLoader
from aml_database_setup import AMLDatabaseManager

app = Flask(__name__)

# –°–æ–∑–¥–∞–µ–º Blueprint –¥–ª—è API —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º /api
api_bp = Blueprint('api', __name__, url_prefix='/api')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º
CORS(app, resources={
    r"/*": {
        "origins": ["http://localhost:3000"],
        "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS", "HEAD", "PATCH"],
        "allow_headers": [
            "Content-Type", "Authorization", "Upload-Offset", "Upload-Length", 
            "Tus-Resumable", "X-File-Name", "X-File-Size", "X-Upload-Time", 
            "X-Department", "Upload-Metadata"
        ],
        "expose_headers": [
            "Location", "Upload-Offset", "Tus-Resumable", "Upload-Length",
            "Tus-Version", "Tus-Max-Size", "Tus-Extension", "Upload-Metadata"
        ]
    }
})

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
UPLOAD_FOLDER = 'uploads'
CHUNKS_FOLDER = 'chunks'
PROCESSING_FOLDER = 'processing'
RESULTS_FOLDER = 'results'

# –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–ø–∫–∏
for folder in [UPLOAD_FOLDER, CHUNKS_FOLDER, PROCESSING_FOLDER, RESULTS_FOLDER]:
    os.makedirs(folder, exist_ok=True)

# –•—Ä–∞–Ω–∏–ª–∏—â–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∑–∞–≥—Ä—É–∑–∫–∞—Ö (–≤ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ - –ë–î)
uploads = {}
processing_tasks = {}

# === –ù–û–í–û–ï: –ø—É—Ç—å –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ë–î —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ ===
latest_db_path = 'aml_system_e840b2937714940f.db'  # –∏—Å–ø–æ–ª—å–∑—É–µ–º –ë–î —Å –ø–æ–ª–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º

test_transactions = []  # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π

def run_enhanced_analysis(json_filepath: str, db_filepath: str) -> str:
    """
    –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ UnifiedAMLPipeline
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    try:
        print(f"üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {json_filepath} -> {db_filepath}")
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î
        loader = AMLJSONDataLoader()
        loader.load_json_to_database(json_filepath, db_filepath)
        
        # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ pipeline
        pipeline = UnifiedAMLPipeline()
        pipeline._initialize_database(db_filepath)
        
        # 3. –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –∏–∑ –ë–î
        with AMLDatabaseManager(db_filepath) as db:
            cursor = db.connection.cursor()
            cursor.execute("""
                SELECT transaction_id, amount, amount_kzt, sender_id, beneficiary_id, 
                       transaction_date, sender_country, beneficiary_country,
                       sender_name, beneficiary_name, purpose_text, 
                       final_risk_score, is_suspicious
                FROM transactions 
                ORDER BY transaction_date DESC
            """)
            transactions = [dict(row) for row in cursor.fetchall()]
        
        print(f"üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(transactions)} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π...")
        
        # 4. –ê–Ω–∞–ª–∏–∑ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π —Å –Ω–æ–≤—ã–º pipeline
        results = []
        total_transactions = len(transactions)
        
        for i, tx in enumerate(transactions):
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 1000 —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
            if i % 1000 == 0:
                print(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{total_transactions} ({i/total_transactions*100:.1f}%)")
            
            try:
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                transaction = {
                    'transaction_id': tx['transaction_id'],
                    'amount': float(tx['amount']),
                    'amount_kzt': float(tx.get('amount_kzt', tx['amount'])),
                    'sender_id': tx['sender_id'],
                    'beneficiary_id': tx['beneficiary_id'],
                    'sender_name': tx.get('sender_name', ''),
                    'beneficiary_name': tx.get('beneficiary_name', ''),
                    'purpose_text': tx.get('purpose_text', ''),
                    'transaction_date': tx['transaction_date'],
                    'date': datetime.strptime(tx['transaction_date'], '%Y-%m-%d %H:%M:%S'),
                    'sender_country': tx.get('sender_country', 'KZ'),
                    'beneficiary_country': tx.get('beneficiary_country', 'KZ'),
                    'final_risk_score': float(tx.get('final_risk_score', 0)),
                    'is_suspicious': bool(tx.get('is_suspicious', False))
                }
                
                # –ê–Ω–∞–ª–∏–∑ —Å –Ω–æ–≤—ã–º pipeline
                result = pipeline._analyze_single_transaction(transaction)
                
                results.append({
                    'transaction_id': tx['transaction_id'],
                    'overall_risk': result.overall_risk,
                    'risk_category': result.risk_category,
                    'transaction_risk': result.transaction_risk,
                    'customer_risk': result.customer_risk,
                    'network_risk': result.network_risk,
                    'behavioral_risk': result.behavioral_risk,
                    'geographic_risk': result.geographic_risk,
                    'suspicious_flags': result.suspicious_flags,
                    'explanations': result.explanations
                })
                
                if (i + 1) % 100 == 0:
                    print(f"  ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{len(transactions)} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")
                    
            except Exception as e:
                print(f"  ‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ {tx['transaction_id']}: {e}")
                continue
        
        # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results_file = os.path.join(RESULTS_FOLDER, f'enhanced_analysis_{int(datetime.now().timestamp())}.json')
        
        summary = {
            'timestamp': datetime.now().isoformat(),
            'total_transactions': len(transactions),
            'analyzed_transactions': len(results),
            'high_risk_count': len([r for r in results if r['overall_risk'] >= 4.0]),
            'medium_risk_count': len([r for r in results if 2.0 <= r['overall_risk'] < 4.0]),
            'low_risk_count': len([r for r in results if r['overall_risk'] < 2.0]),
            'average_risk': sum(r['overall_risk'] for r in results) / len(results) if results else 0,
            'pipeline_version': 'unified_enhanced',
            'results': results[:100]  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—ã–µ 100 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {results_file}")
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {len(results)} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π, —Å—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫: {summary['average_risk']:.2f}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è API
        global latest_db_path
        latest_db_path = db_filepath
        
        return results_file
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —É–ª—É—á—à–µ–Ω–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ: {e}")
        # Fallback –Ω–∞ —Å—Ç–∞—Ä—É—é —Å–∏—Å—Ç–µ–º—É
        return run_full_analysis(json_filepath, db_filepath)

# –ù–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è TUS-–∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∫ OPTIONS –æ—Ç–≤–µ—Ç–∞–º
@app.after_request
def add_tus_headers(response):
    if request.method == 'OPTIONS' and request.path.startswith('/upload'):
        response.headers['Tus-Resumable'] = '1.0.0'
        response.headers['Tus-Version'] = '1.0.0'
        response.headers['Tus-Extension'] = 'creation,creation-with-upload'
        response.headers['Tus-Max-Size'] = '1073741824'  # 1GB
    return response

# –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è TUS –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ (–±–µ–∑ /api –ø—Ä–µ—Ñ–∏–∫—Å–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Uppy)
@app.route('/upload', methods=['POST', 'HEAD', 'PATCH']) # <-- –£–±—Ä–∞–ª–∏ OPTIONS
@app.route('/upload/<file_id>', methods=['POST', 'HEAD', 'PATCH']) # <-- –£–±—Ä–∞–ª–∏ OPTIONS
def tus_upload(file_id=None):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ TUS –∑–∞–≥—Ä—É–∑–æ–∫"""
    
    print(f"TUS –∑–∞–ø—Ä–æ—Å: {request.method} {request.path}")
    print(f"–ó–∞–≥–æ–ª–æ–≤–∫–∏: {dict(request.headers)}")
    
    if request.method == 'HEAD':
        if not file_id or file_id not in uploads:
            return '', 404
        
        upload_info = uploads[file_id]
        response = make_response('')
        response.headers['Upload-Offset'] = str(upload_info['offset'])
        response.headers['Upload-Length'] = str(upload_info['size'])
        response.headers['Tus-Resumable'] = '1.0.0'
        response.headers['Cache-Control'] = 'no-store'
        return response
    
    if request.method == 'POST':
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
        upload_length = request.headers.get('Upload-Length')
        upload_metadata = request.headers.get('Upload-Metadata', '')
        
        if not upload_length:
            return jsonify({'error': 'Upload-Length header required'}), 400
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
        file_id = hashlib.sha256(f"{datetime.now().isoformat()}{upload_metadata}".encode()).hexdigest()[:16]
        
        # –ü–∞—Ä—Å–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {}
        if upload_metadata:
            for item in upload_metadata.split(','):
                if ' ' in item:
                    key, value = item.split(' ', 1)
                    try:
                        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º base64
                        import base64
                        decoded_value = base64.b64decode(value).decode('utf-8')
                        metadata[key] = decoded_value
                    except:
                        metadata[key] = value
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∑–∫–µ
        uploads[file_id] = {
            'id': file_id,
            'size': int(upload_length),
            'offset': 0,
            'metadata': metadata,
            'created_at': datetime.now().isoformat(),
            'chunks_path': os.path.join(CHUNKS_FOLDER, file_id)
        }
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —á–∞–Ω–∫–æ–≤
        os.makedirs(uploads[file_id]['chunks_path'], exist_ok=True)
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
        response = make_response('', 201)
        response.headers['Location'] = f'/upload/{file_id}'
        response.headers['Tus-Resumable'] = '1.0.0'
        
        return response
    
    if request.method == 'PATCH':
        if not file_id or file_id not in uploads:
            return jsonify({'error': 'Upload not found'}), 404
        
        upload_info = uploads[file_id]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º offset
        upload_offset = int(request.headers.get('Upload-Offset', 0))
        if upload_offset != upload_info['offset']:
            return jsonify({'error': 'Offset mismatch'}), 409
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫
        chunk_path = os.path.join(upload_info['chunks_path'], f"chunk_{upload_offset}")
        
        with open(chunk_path, 'wb') as f:
            chunk_data = request.get_data()
            f.write(chunk_data)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º offset
        upload_info['offset'] += len(chunk_data)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞
        if upload_info['offset'] >= upload_info['size']:
            # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∞–π–ª –∏–∑ —á–∞–Ω–∫–æ–≤
            final_filename = upload_info['metadata'].get('filename', f"{file_id}.dat")
            final_path = os.path.join(UPLOAD_FOLDER, secure_filename(final_filename))
            
            with open(final_path, 'wb') as final_file:
                chunk_files = sorted(os.listdir(upload_info['chunks_path']), key=lambda x: int(x.split('_')[1])) # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
                for chunk_file in chunk_files:
                    chunk_path = os.path.join(upload_info['chunks_path'], chunk_file)
                    with open(chunk_path, 'rb') as chunk:
                        final_file.write(chunk.read())
            
            # –û—á–∏—â–∞–µ–º —á–∞–Ω–∫–∏
            shutil.rmtree(upload_info['chunks_path'])
            
            # –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
            upload_info['status'] = 'processing'
            upload_info['final_path'] = final_path
            
            # 3. –ó–∞–º–µ–Ω—è–µ–º —Å–∏–º—É–ª—è—Ü–∏—é –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ –≤ —Ñ–æ–Ω–æ–≤–æ–º –ø–æ—Ç–æ–∫–µ
            task_id = file_id # –ò—Å–ø–æ–ª—å–∑—É–µ–º file_id –∫–∞–∫ ID –∑–∞–¥–∞—á–∏
            processing_tasks[task_id] = {
                'status': 'queued',
                'progress': 0,
                'message': '–§–∞–π–ª –ø–æ—Å—Ç–∞–≤–ª–µ–Ω –≤ –æ—á–µ—Ä–µ–¥—å –Ω–∞ –∞–Ω–∞–ª–∏–∑...'
            }
            
            # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞
            db_path = latest_db_path  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            analysis_thread = threading.Thread(
                target=run_analysis_and_update_status,
                args=(task_id, final_path, db_path)
            )
            analysis_thread.start()
            
            print(f"–§–∞–π–ª {final_filename} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω, –∑–∞–ø—É—â–µ–Ω –∞–Ω–∞–ª–∏–∑ –≤ —Ñ–æ–Ω–æ–≤–æ–º –ø–æ—Ç–æ–∫–µ (ID: {task_id})")
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
        response = make_response('', 204)
        response.headers['Upload-Offset'] = str(upload_info['offset'])
        response.headers['Tus-Resumable'] = '1.0.0'
        
        return response
    
    return jsonify({'error': 'Method not allowed'}), 405

def run_analysis_and_update_status(task_id, json_filepath, db_filepath):
    """
    –§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤ –ø–æ—Ç–æ–∫–µ.
    –û–Ω–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∞–Ω–∞–ª–∏–∑ –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏.
    """
    try:
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "–≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ"
        processing_tasks[task_id]['status'] = 'processing'
        processing_tasks[task_id]['message'] = '–ò–¥–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö...'
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ—Ñ–∏–ª—è–º–∏
        report_path = run_enhanced_analysis(json_filepath, db_filepath)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "–∑–∞–≤–µ—Ä—à–µ–Ω–æ"
        processing_tasks[task_id]['status'] = 'completed'
        processing_tasks[task_id]['message'] = '–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω.'
        processing_tasks[task_id]['progress'] = 100
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∑–¥–µ—Å—å –º–æ–∂–Ω–æ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–∏—Ç—å –ø—É—Ç—å –∫ –æ—Ç—á–µ—Ç—É –∏–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        with open(report_path, 'r', encoding='utf-8') as f:
            results_summary = json.load(f)

        processing_tasks[task_id]['results'] = results_summary

    except Exception as e:
        # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        print(f"–û–®–ò–ë–ö–ê –≤ –ø–æ—Ç–æ–∫–µ –∞–Ω–∞–ª–∏–∑–∞ (ID: {task_id}): {e}")
        processing_tasks[task_id]['status'] = 'failed'
        processing_tasks[task_id]['message'] = f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}"

    finally:
        # –ü—É—Ç—å –∫ –ë–î –æ—Å—Ç–∞–µ—Ç—Å—è –Ω–µ–∏–∑–º–µ–Ω–Ω—ã–º
        pass

@api_bp.route('/processing-status/<file_id>', methods=['GET'])
def get_processing_status(file_id):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞"""
    if file_id not in processing_tasks:
        return jsonify({'error': 'Task not found'}), 404
    
    task = processing_tasks[file_id]
    
    # –°–∏–º—É–ª—è—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–Ω–∞, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –≤ –ø–æ—Ç–æ–∫–µ
    # (–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª–µ–µ –≥—Ä–∞–Ω—É–ª—è—Ä–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ run_full_analysis –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏)

    return jsonify(task)

@api_bp.route('/files', methods=['GET'])
def get_files():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    files = []
    for file_id, upload_info in uploads.items():
        files.append({
            'id': file_id,
            'filename': upload_info['metadata'].get('filename', 'Unknown'),
            'size': upload_info['size'],
            'created_at': upload_info['created_at'],
            'status': upload_info.get('status', 'uploading')
        })
    
    return jsonify({
        'files': files,
        'total': len(files)
    })

@api_bp.route('/analytics/dashboard', methods=['GET'])
def get_dashboard_data():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–∞—à–±–æ—Ä–¥–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º unified pipeline"""
    global latest_db_path
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    enhanced_results = get_latest_enhanced_results()
    
    if enhanced_results:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ unified pipeline
        return jsonify({
            'summary': {
                'total_transactions': enhanced_results.get('analyzed_transactions', 0),
                'flagged_transactions': enhanced_results.get('high_risk_count', 0) + enhanced_results.get('medium_risk_count', 0),
                'total_amount': 26100088012585,  # –û–±—â–∞—è —Å—É–º–º–∞ –∏–∑ –ë–î
                'alerts_pending': 0
            },
            'risk_distribution': {
                'high': enhanced_results.get('high_risk_count', 0),
                'medium': enhanced_results.get('medium_risk_count', 0),
                'low': enhanced_results.get('low_risk_count', 0)
            },
            'recent_alerts': [
                {
                    'id': 'alert_1',
                    'type': '–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è',
                    'amount': 15000000,
                    'date': datetime.now().isoformat(),
                    'risk_level': 'high'
                },
                {
                    'id': 'alert_2', 
                    'type': '–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã',
                    'amount': 8500000,
                    'date': datetime.now().isoformat(),
                    'risk_level': 'medium'
                },
                {
                    'id': 'alert_3',
                    'type': '–ù–µ—Ç–∏–ø–∏—á–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ',
                    'amount': 3200000,
                    'date': datetime.now().isoformat(),
                    'risk_level': 'high'
                }
            ],
            'trends': {
                'daily_volumes': [
                    {'date': '2024-06-23', 'count': 8500},
                    {'date': '2024-06-24', 'count': 9200},
                    {'date': '2024-06-25', 'count': 7800},
                    {'date': '2024-06-26', 'count': 10100},
                    {'date': '2024-06-27', 'count': 9500},
                    {'date': '2024-06-28', 'count': 11200},
                    {'date': '2024-06-29', 'count': 10800}
                ]
            },
            'last_updated': datetime.now().isoformat()
        })
    
    # Fallback –∫ —Å—Ç–∞—Ä—ã–º –¥–∞–Ω–Ω—ã–º –ë–î, –µ—Å–ª–∏ –Ω–µ—Ç enhanced —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if latest_db_path and os.path.exists(latest_db_path):
        from aml_database_setup import AMLDatabaseManager
        db = AMLDatabaseManager(db_path=latest_db_path)
        stats = db.get_system_statistics()
        db.close()

        return jsonify({
            'summary': {
                'total_transactions': stats['transactions']['total_transactions'],
                'flagged_transactions': stats['transactions']['suspicious_transactions'],
                'total_amount': stats['transactions']['total_volume'],
                'alerts_pending': stats['alerts']['new_alerts']
            },
            'risk_distribution': {
                'high': stats['transactions']['suspicious_transactions'],
                'medium': 0,
                'low': stats['transactions']['total_transactions'] - stats['transactions']['suspicious_transactions']
            },
            'recent_alerts': [
                {
                    'id': 'alert_1',
                    'type': '–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è',
                    'amount': 15000000,
                    'date': datetime.now().isoformat(),
                    'risk_level': 'high'
                },
                {
                    'id': 'alert_2', 
                    'type': '–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã',
                    'amount': 8500000,
                    'date': datetime.now().isoformat(),
                    'risk_level': 'medium'
                },
                {
                    'id': 'alert_3',
                    'type': '–ù–µ—Ç–∏–ø–∏—á–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ',
                    'amount': 3200000,
                    'date': datetime.now().isoformat(),
                    'risk_level': 'high'
                }
            ],
            'trends': {
                'daily_volumes': [
                    {'date': '2024-06-23', 'count': 8500},
                    {'date': '2024-06-24', 'count': 9200},
                    {'date': '2024-06-25', 'count': 7800},
                    {'date': '2024-06-26', 'count': 10100},
                    {'date': '2024-06-27', 'count': 9500},
                    {'date': '2024-06-28', 'count': 11200},
                    {'date': '2024-06-29', 'count': 10800}
                ]
            },
            'last_updated': datetime.now().isoformat()
        })

    # –§–æ–ª–±–µ–∫ –∫ –∑–∞–≥–ª—É—à–∫–µ, –µ—Å–ª–∏ –Ω–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–π –ë–î
    return jsonify({
        'summary': {
            'total_transactions': 0,
            'flagged_transactions': 0,
            'total_amount': 0,
            'alerts_pending': 0
        },
        'risk_distribution': {
            'high': 0,
            'medium': 0,
            'low': 0
        },
        'recent_alerts': [],
        'trends': {
            'daily_volumes': []
        },
        'last_updated': datetime.now().isoformat()
    })

@api_bp.route('/analytics/network-graph', methods=['GET'])
def get_network_graph():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–µ—Ç–µ–≤–æ–≥–æ –≥—Ä–∞—Ñ–∞ –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö —Å–≤—è–∑–µ–π"""
    global latest_db_path
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
    limit = int(request.args.get('limit', 100))
    min_amount = float(request.args.get('min_amount', 1000000))  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—É–º–º–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
    days_back = int(request.args.get('days', 30))
    
    if latest_db_path and os.path.exists(latest_db_path):
        try:
            from aml_database_setup import AMLDatabaseManager
            with AMLDatabaseManager(db_path=latest_db_path) as db:
                cursor = db.connection.cursor()
                
                # –ü–æ–ª—É—á–∞–µ–º —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∫–ª–∏–µ–Ω—Ç–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
                cursor.execute('''
                SELECT 
                    sender_id,
                    sender_name,
                    beneficiary_id, 
                    beneficiary_name,
                    COUNT(*) as transaction_count,
                    SUM(amount_kzt) as total_amount,
                    AVG(final_risk_score) as avg_risk_score,
                    MAX(final_risk_score) as max_risk_score,
                    GROUP_CONCAT(DISTINCT 
                        CASE WHEN is_suspicious THEN transaction_id END
                    ) as suspicious_transactions
                FROM transactions
                WHERE sender_id IS NOT NULL 
                    AND beneficiary_id IS NOT NULL
                    AND sender_id != beneficiary_id
                    AND amount_kzt >= ?
                    AND datetime(transaction_date) >= datetime('now', '-{} days')
                GROUP BY sender_id, beneficiary_id
                HAVING total_amount >= ?
                ORDER BY total_amount DESC, avg_risk_score DESC
                LIMIT ?
                '''.format(days_back), (min_amount, min_amount, limit))
                
                connections = cursor.fetchall()
                
                # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —É–∑–ª–æ–≤ –∏ —Å–≤—è–∑–µ–π
                nodes = {}
                edges = []
                
                for conn in connections:
                    sender_id, sender_name, beneficiary_id, beneficiary_name, \
                    tx_count, total_amount, avg_risk, max_risk, suspicious_txs = conn
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —É–∑–ª—ã
                    if sender_id not in nodes:
                        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–ª–∏–µ–Ω—Ç–µ
                        cursor.execute('''
                        SELECT COUNT(*) as total_tx, SUM(amount_kzt) as total_vol, 
                               AVG(final_risk_score) as client_risk
                        FROM transactions 
                        WHERE sender_id = ? OR beneficiary_id = ?
                        ''', (sender_id, sender_id))
                        
                        client_stats = cursor.fetchone()
                        nodes[sender_id] = {
                            'id': sender_id,
                            'name': sender_name or f"–ö–ª–∏–µ–Ω—Ç {sender_id}",
                            'type': 'sender',
                            'total_transactions': client_stats[0] if client_stats else 0,
                            'total_volume': client_stats[1] if client_stats else 0,
                            'risk_score': client_stats[2] if client_stats else 0,
                            'size': min(50, max(10, (client_stats[1] or 0) / 10000000))  # –†–∞–∑–º–µ—Ä —É–∑–ª–∞
                        }
                    
                    if beneficiary_id not in nodes:
                        cursor.execute('''
                        SELECT COUNT(*) as total_tx, SUM(amount_kzt) as total_vol,
                               AVG(final_risk_score) as client_risk  
                        FROM transactions
                        WHERE sender_id = ? OR beneficiary_id = ?
                        ''', (beneficiary_id, beneficiary_id))
                        
                        client_stats = cursor.fetchone()
                        nodes[beneficiary_id] = {
                            'id': beneficiary_id,
                            'name': beneficiary_name or f"–ö–ª–∏–µ–Ω—Ç {beneficiary_id}",
                            'type': 'beneficiary',
                            'total_transactions': client_stats[0] if client_stats else 0,
                            'total_volume': client_stats[1] if client_stats else 0,
                            'risk_score': client_stats[2] if client_stats else 0,
                            'size': min(50, max(10, (client_stats[1] or 0) / 10000000))
                        }
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑—å
                    edge = {
                        'source': sender_id,
                        'target': beneficiary_id,
                        'transaction_count': tx_count,
                        'total_amount': total_amount,
                        'avg_risk_score': avg_risk,
                        'max_risk_score': max_risk,
                        'is_suspicious': bool(suspicious_txs),
                        'suspicious_transactions': suspicious_txs.split(',') if suspicious_txs else [],
                        'weight': min(10, max(1, tx_count / 5))  # –¢–æ–ª—â–∏–Ω–∞ —Å–≤—è–∑–∏
                    }
                    edges.append(edge)
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç—å —É–∑–ª–æ–≤
                node_connections = {}
                for edge in edges:
                    node_connections[edge['source']] = node_connections.get(edge['source'], 0) + 1
                    node_connections[edge['target']] = node_connections.get(edge['target'], 0) + 1
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã —É–∑–ª–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç–∏
                for node_id, node in nodes.items():
                    centrality = node_connections.get(node_id, 0)
                    node['centrality'] = centrality
                    node['size'] = min(60, max(15, centrality * 5 + node['size']))
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∏—Å–∫–∞
                    risk = node['risk_score']
                    if risk >= 7.0:
                        node['color'] = '#ef4444'  # –ö—Ä–∞—Å–Ω—ã–π - –≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫
                    elif risk >= 4.0:
                        node['color'] = '#f59e0b'  # –û—Ä–∞–Ω–∂–µ–≤—ã–π - —Å—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫
                    elif risk >= 2.0:
                        node['color'] = '#eab308'  # –ñ–µ–ª—Ç—ã–π - –Ω–∏–∑–∫–∏–π —Ä–∏—Å–∫
                    else:
                        node['color'] = '#22c55e'  # –ó–µ–ª–µ–Ω—ã–π - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∏—Å–∫
                
                return jsonify({
                    'nodes': list(nodes.values()),
                    'edges': edges,
                    'statistics': {
                        'total_nodes': len(nodes),
                        'total_edges': len(edges),
                        'high_risk_nodes': len([n for n in nodes.values() if n['risk_score'] >= 7.0]),
                        'suspicious_connections': len([e for e in edges if e['is_suspicious']]),
                        'total_volume': sum(e['total_amount'] for e in edges),
                        'date_range': f"–ü–æ—Å–ª–µ–¥–Ω–∏–µ {days_back} –¥–Ω–µ–π",
                        'min_amount_filter': min_amount
                    },
                    'last_updated': datetime.now().isoformat()
                })
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–µ—Ç–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
            return jsonify({
                'nodes': [],
                'edges': [],
                'statistics': {
                    'total_nodes': 0,
                    'total_edges': 0,
                    'high_risk_nodes': 0,
                    'suspicious_connections': 0,
                    'total_volume': 0
                },
                'error': str(e),
                'last_updated': datetime.now().isoformat()
            })
    
    # –ó–∞–≥–ª—É—à–∫–∞ –µ—Å–ª–∏ –Ω–µ—Ç –ë–î
    return jsonify({
        'nodes': [],
        'edges': [],
        'statistics': {
            'total_nodes': 0,
            'total_edges': 0,
            'high_risk_nodes': 0,
            'suspicious_connections': 0,
            'total_volume': 0
        },
        'last_updated': datetime.now().isoformat()
    })

@api_bp.route('/health', methods=['GET'])
def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–µ—Ä–∞"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'version': '1.0.0'
    })

@api_bp.route('/')
def api_index():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ API"""
    return jsonify({
        'service': '–ê–§–ú –†–ö - API —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π',
        'version': '1.0.0',
        'endpoints': {
            'health': '/api/health',
            'upload': '/upload',
            'files': '/api/files',
            'processing_status': '/api/processing-status/<file_id>',
            'dashboard': '/api/analytics/dashboard',
            'risk_analysis': '/api/analytics/risk-analysis',
            'network_graph': '/api/analytics/network-graph',
            'transactions': '/api/transactions',
            'transaction_details': '/api/transactions/<transaction_id>',
            'transaction_review': '/api/transactions/<transaction_id>/review',
            'transactions_export': '/api/transactions/export'
        },
        'frontend_url': 'http://localhost:3000'
    })

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
def generate_test_transactions(count=100):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    transactions = []
    
    # –°–ø–∏—Å–∫–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    names = [
        '–¢–û–û "–ö–∞–∑–¢—Ä–µ–π–¥"', '–ê–û "–ê–ª–º–∞–ò–Ω–≤–µ—Å—Ç"', '–ò–ü –ò–≤–∞–Ω–æ–≤ –ò.–ò.', '–¢–û–û "–ù—É—Ä–°—Ç—Ä–æ–π"',
        '–ê–û "–ö–∞–∑–ú—É–Ω–∞–π–ì–∞–∑"', '–¢–û–û "–ê—Å—Ç–∞–Ω–∞–ë–∏–∑–Ω–µ—Å"', '–ò–ü –ü–µ—Ç—Ä–æ–≤ –ü.–ü.', '–ê–û "–ö–∞–∑–∞—Ö–¢–µ–ª–µ–∫–æ–º"',
        '–¢–û–û "–ê–ª–º–∞—Ç—ã–¢—Ä–∞–Ω—Å"', '–ò–ü –°–∏–¥–æ—Ä–æ–≤ –°.–°.'
    ]
    
    banks = ['–•–∞–ª—ã–∫ –ë–∞–Ω–∫', 'Kaspi Bank', '–ë–¶–ö', 'Forte Bank', 'Jusan Bank']
    risk_reasons = [
        '–ö—Ä—É–ø–Ω–∞—è –Ω–∞–ª–∏—á–Ω–∞—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è',
        '–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã',
        '–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ö–µ–º–∞',
        '–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ—Ñ–∏–ª—é –∫–ª–∏–µ–Ω—Ç–∞',
        '–°–≤—è–∑—å —Å –≤—ã—Å–æ–∫–æ—Ä–∏—Å–∫–æ–≤–æ–π —é—Ä–∏—Å–¥–∏–∫—Ü–∏–µ–π'
    ]
    
    for i in range(count):
        risk_level = random.choices(['low', 'medium', 'high'], weights=[0.7, 0.2, 0.1])[0]
        amount = random.uniform(100000, 50000000) if risk_level == 'high' else random.uniform(10000, 5000000)
        
        transaction = {
            'id': f'TRX-2024-{str(i+1).zfill(6)}',
            'reference_id': f'REF-{random.randint(100000, 999999)}',
            'date': f'2024-01-{random.randint(1, 15):02d}T{random.randint(0, 23):02d}:{random.randint(0, 59):02d}:00',
            'sender': {
                'name': random.choice(names),
                'account': f'KZ{random.randint(100, 999)}XXX{random.randint(1000000, 9999999)}',
                'bank': random.choice(banks),
                'type': random.choice(['–Æ—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ', '–ò–ü', '–§–∏–∑–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ'])
            },
            'receiver': {
                'name': random.choice(names),
                'account': f'KZ{random.randint(100, 999)}XXX{random.randint(1000000, 9999999)}',
                'bank': random.choice(banks),
                'type': random.choice(['–Æ—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ', '–ò–ü', '–§–∏–∑–∏—á–µ—Å–∫–æ–µ –ª–∏—Ü–æ'])
            },
            'amount': round(amount, 2),
            'currency': 'KZT',
            'risk_level': risk_level,
            'risk_score': random.uniform(0.7, 0.95) if risk_level == 'high' else random.uniform(0.3, 0.7),
            'risk_reasons': random.sample(risk_reasons, k=random.randint(1, 3)) if risk_level != 'low' else [],
            'status': random.choice(['pending', 'reviewing', 'cleared', 'reported']),
            'description': f'–ü–µ—Ä–µ–≤–æ–¥ —Å—Ä–µ–¥—Å—Ç–≤ –ø–æ –¥–æ–≥–æ–≤–æ—Ä—É ‚Ññ{random.randint(1000, 9999)}'
        }
        
        transactions.append(transaction)
    
    return sorted(transactions, key=lambda x: x['date'], reverse=True)

@api_bp.route('/transactions', methods=['GET'])
def get_transactions():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ë–î —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π"""
    global latest_db_path

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
    page = int(request.args.get('page', 1))
    limit = int(request.args.get('limit', 50))

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏ –ø–æ–∏—Å–∫–∞
    risk_level = request.args.get('risk_level', '')
    start_date = request.args.get('start_date', '')
    end_date = request.args.get('end_date', '')
    search = request.args.get('search', '').lower()

    transactions_source = []

    if latest_db_path and os.path.exists(latest_db_path):
        from aml_database_setup import AMLDatabaseManager
        db = AMLDatabaseManager(db_path=latest_db_path)
        transactions_source = db.get_all_transactions()
        db.close()

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —É—Ä–æ–≤–Ω—é —Ä–∏—Å–∫–∞
    if risk_level:
        transactions_source = [t for t in transactions_source if str(t.get('final_risk_score', 0)).lower() == risk_level.lower() or t.get('risk_level') == risk_level]

    # –ü–æ–∏—Å–∫ –ø–æ —Å—Ç—Ä–æ–∫–µ
    if search:
        transactions_source = [t for t in transactions_source if search in str(t).lower()]

    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞—Ç –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–∞–º
    def parse_date(val):
        try:
            return datetime.fromisoformat(val) if isinstance(val, str) else val
        except ValueError:
            return None

    if start_date:
        start_dt = parse_date(start_date)
        transactions_source = [t for t in transactions_source if parse_date(t.get('transaction_date') or t.get('date')) >= start_dt]
    if end_date:
        end_dt = parse_date(end_date)
        transactions_source = [t for t in transactions_source if parse_date(t.get('transaction_date') or t.get('date')) <= end_dt]

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ (desc)
    transactions_source.sort(key=lambda x: parse_date(x.get('transaction_date') or x.get('date')) or datetime.min, reverse=True)

    total = len(transactions_source)
    start = (page - 1) * limit
    end = start + limit

    return jsonify({
        'transactions': transactions_source[start:end],
        'pagination': {
            'page': page,
            'limit': limit,
            'total': total,
            'pages': (total + limit - 1) // limit
        }
    })

@api_bp.route('/transactions/<transaction_id>', methods=['GET'])
def get_transaction_details(transaction_id):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∏–∑ –ë–î"""
    global latest_db_path

    transaction_data = None
    if latest_db_path and os.path.exists(latest_db_path):
        from aml_database_setup import AMLDatabaseManager
        db = AMLDatabaseManager(db_path=latest_db_path)
        cursor = db.connection.cursor()
        cursor.execute("SELECT * FROM transactions WHERE transaction_id = ?", (transaction_id,))
        row = cursor.fetchone()
        if row:
            transaction_data = dict(row)
        db.close()

    if not transaction_data:
        return jsonify({'error': 'Transaction not found'}), 404

    return jsonify(transaction_data)

@api_bp.route('/transactions/<transaction_id>/review', methods=['POST'])
def mark_transaction_reviewed(transaction_id):
    """–û—Ç–º–µ—á–∞–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é –∫–∞–∫ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—É—é"""
    global latest_db_path
    
    if not latest_db_path or not os.path.exists(latest_db_path):
        return jsonify({'error': 'Database not initialized'}), 503
    
    # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –∑–¥–µ—Å—å –±—ã –æ–±–Ω–æ–≤–ª—è–ª–∞—Å—å –ë–î
    review_data = request.json
    print(f"–¢—Ä–∞–Ω–∑–∞–∫—Ü–∏—è {transaction_id} –æ—Ç–º–µ—á–µ–Ω–∞ –∫–∞–∫ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è")
    print(f"–ü—Ä–æ–≤–µ—Ä–∏–ª: {review_data.get('reviewedBy')}")
    print(f"–í—Ä–µ–º—è: {review_data.get('reviewedAt')}")
    print(f"–ó–∞–º–µ—Ç–∫–∏: {review_data.get('notes')}")
    
    return jsonify({'status': 'success', 'message': 'Transaction marked as reviewed'})

@api_bp.route('/transactions/export', methods=['GET'])
def export_transactions():
    """–≠–∫—Å–ø–æ—Ä—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –≤ CSV –∏–∑ —Ä–µ–∞–ª—å–Ω–æ–π –ë–î –ª–∏–±–æ –∏–∑ –º–æ–∫-–¥–∞–Ω–Ω—ã—Ö"""
    import io
    import csv
    global latest_db_path

    risk_level = request.args.get('riskLevel', '')

    transactions_source = []
    if latest_db_path and os.path.exists(latest_db_path):
        from aml_database_setup import AMLDatabaseManager
        db = AMLDatabaseManager(db_path=latest_db_path)
        transactions_source = db.get_all_transactions()
        db.close()

    if risk_level:
        transactions_source = [t for t in transactions_source if str(t.get('final_risk_score', 0)).lower() == risk_level.lower() or t.get('risk_level') == risk_level]

    # –°–æ–∑–¥–∞–µ–º CSV
    output = io.StringIO()
    writer = csv.writer(output)
    
    # –ó–∞–≥–æ–ª–æ–≤–∫–∏
    writer.writerow([
        'ID —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏', '–î–∞—Ç–∞', '–û—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—å', '–ë–∞–Ω–∫ –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—è',
        '–ü–æ–ª—É—á–∞—Ç–µ–ª—å', '–ë–∞–Ω–∫ –ø–æ–ª—É—á–∞—Ç–µ–ª—è', '–°—É–º–º–∞', '–£—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞'
    ])
    
    # –î–∞–Ω–Ω—ã–µ
    for t in transactions_source[:100]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 100 –∑–∞–ø–∏—Å—è–º–∏
        writer.writerow([
            t.get('reference_id') or t.get('transaction_id'),
            t.get('date') or t.get('transaction_date'),
            t.get('sender', {}).get('name') or t.get('sender_name'),
            t.get('sender', {}).get('bank') or t.get('sender_bank_bic'),
            t.get('receiver', {}).get('name') or t.get('beneficiary_name'),
            t.get('receiver', {}).get('bank') or t.get('beneficiary_bank_bic'),
            t.get('amount') or t.get('amount_kzt'),
            t.get('risk_level') or ('HIGH' if t.get('is_suspicious') else 'LOW')
        ])
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ —Ñ–∞–π–ª
    output.seek(0)
    response = make_response(output.getvalue())
    response.headers['Content-Disposition'] = 'attachment; filename=transactions.csv'
    response.headers['Content-Type'] = 'text/csv; charset=utf-8'
    
    return response

def get_latest_enhanced_results():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    try:
        results_files = glob.glob(os.path.join(RESULTS_FOLDER, 'enhanced_existing_*.json'))
        if results_files:
            # –ë–µ—Ä–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π —Ñ–∞–π–ª
            latest_file = max(results_files, key=os.path.getmtime)
            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
    return None

def get_enhanced_risk_analysis(enhanced_data):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è API"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        risk_level_filter = request.args.get('riskLevel', 'all')
        analysis_type = request.args.get('analysisType', 'all')
        
        results = enhanced_data.get('results', [])
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —É—Ä–æ–≤–Ω—é —Ä–∏—Å–∫–∞
        filtered_results = []
        for result in results:
            risk_score = result.get('overall_risk', 0)
            
            if risk_level_filter == 'high' and risk_score <= 5.0:
                continue
            elif risk_level_filter == 'medium' and (risk_score <= 3.0 or risk_score > 5.0):
                continue
            elif risk_level_filter == 'low' and risk_score > 3.0:
                continue
            
            filtered_results.append(result)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–∏–ø—É –∞–Ω–∞–ª–∏–∑–∞
        if analysis_type != 'all':
            type_filtered = []
            for result in filtered_results:
                explanations = result.get('explanations', [])
                should_include = False
                
                for explanation in explanations:
                    if isinstance(explanation, str):
                        exp_lower = explanation.lower()
                        
                        if analysis_type == 'transactional' and any(keyword in exp_lower for keyword in ['—Ç—Ä–∞–Ω–∑–∞–∫—Ü', '—Å—É–º–º–∞', '–≤—Ä–µ–º—è']):
                            should_include = True
                            break
                        elif analysis_type == 'behavioral' and '–ø–æ–≤–µ–¥–µ–Ω–∏–µ' in exp_lower:
                            should_include = True
                            break
                        elif analysis_type == 'network' and any(keyword in exp_lower for keyword in ['—Å–µ—Ç—å', '—Å—Ö–µ–º–∞']):
                            should_include = True
                            break
                        elif analysis_type == 'customer' and any(keyword in exp_lower for keyword in ['–∫–ª–∏–µ–Ω—Ç', '–∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç']):
                            should_include = True
                            break
                        elif analysis_type == 'geographic' and any(keyword in exp_lower for keyword in ['–≥–µ–æ–≥—Ä–∞—Ñ', '—Å—Ç—Ä–∞–Ω–∞']):
                            should_include = True
                            break
                
                if should_include:
                    type_filtered.append(result)
            
            filtered_results = type_filtered
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –∞–Ω–∞–ª–∏–∑–∞
        analysis_breakdown = {
            'transactional': len([r for r in results if r.get('transaction_risk', 0) > 0.5]),
            'behavioral': len([r for r in results if r.get('behavioral_risk', 0) > 0.5]),
            'network': len([r for r in results if r.get('network_risk', 0) > 0.0]),
            'customer': len([r for r in results if r.get('customer_risk', 0) > 0.5]),
            'geographic': len([r for r in results if r.get('geographic_risk', 0) > 0.3])
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∏—Å–∫–æ–≤
        risk_summary = {
            'high': enhanced_data.get('high_risk_count', 0),
            'medium': enhanced_data.get('medium_risk_count', 0), 
            'low': enhanced_data.get('low_risk_count', 0),
            'total': enhanced_data.get('analyzed_transactions', 0)
        }
        
        # –¢–æ–ø –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Ä–∏—Å–∫–∞
        top_indicators = []
        risk_reasons = {}
        
        for result in results:
            explanations = result.get('explanations', [])
            for explanation in explanations:
                if isinstance(explanation, str):
                    if explanation not in risk_reasons:
                        risk_reasons[explanation] = 0
                    risk_reasons[explanation] += 1
        
        # –¢–æ–ø-5 –ø—Ä–∏—á–∏–Ω
        sorted_reasons = sorted(risk_reasons.items(), key=lambda x: x[1], reverse=True)[:5]
        top_indicators = [{'name': reason, 'count': count} for reason, count in sorted_reasons]
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º
        suspicious_transactions = []
        
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ë–î –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è—Ö
        from aml_database_setup import AMLDatabaseManager
        
        with AMLDatabaseManager(db_path=latest_db_path) as db:
            cursor = db.connection.cursor()
            
            for result in filtered_results[:50]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 50 —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏
                transaction_id = result.get('transaction_id')
                
                # –ü–æ–ª—É—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î
                cursor.execute("""
                    SELECT transaction_date, sender_name, beneficiary_name, amount_kzt, sender_id, beneficiary_id
                    FROM transactions 
                    WHERE transaction_id = ?
                """, (transaction_id,))
                
                db_row = cursor.fetchone()
                if db_row:
                    db_data = dict(db_row)
                    
                    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏—á–∏–Ω –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    suspicious_reasons = []
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
                    explanations = result.get('explanations', [])
                    suspicious_reasons.extend(explanations)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–ª–∞–≥–∏
                    flags = result.get('suspicious_flags', [])
                    suspicious_reasons.extend(flags)
                    
                    suspicious_transactions.append({
                        'transaction_id': transaction_id,
                        'transaction_date': db_data.get('transaction_date'),
                        'sender_name': db_data.get('sender_name'),
                        'beneficiary_name': db_data.get('beneficiary_name'),
                        'amount_kzt': db_data.get('amount_kzt'),
                        'final_risk_score': result.get('overall_risk', 0),
                        'suspicious_reasons': suspicious_reasons,
                        'risk_indicators': {
                            'transaction_risk': result.get('transaction_risk'),
                            'customer_risk': result.get('customer_risk'),
                            'network_risk': result.get('network_risk'),
                            'behavioral_risk': result.get('behavioral_risk'),
                            'geographic_risk': result.get('geographic_risk')
                        },
                        'overall_risk': result.get('overall_risk'),
                        'risk_category': result.get('risk_category'),
                        'explanations': explanations,
                        'suspicious_flags': flags
                    })
        
        return jsonify({
            'risk_summary': risk_summary,
            'analysis_type_breakdown': analysis_breakdown,
            'suspicious_transactions': suspicious_transactions,
            'top_risk_indicators': top_indicators,
            'filters_applied': {
                'risk_level': risk_level_filter,
                'analysis_type': analysis_type,
                'date_range': 'enhanced_analysis'
            },
            'last_updated': enhanced_data.get('timestamp'),
            'data_source': 'enhanced_unified_pipeline'
        })
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
        return jsonify({'error': '–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö'}), 500

@api_bp.route('/analytics/risk-analysis', methods=['GET'])
def get_risk_analysis():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Ä–∏—Å–∫–æ–≤ - –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º"""
    global latest_db_path
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    enhanced_results = get_latest_enhanced_results()
    if enhanced_results:
        return get_enhanced_risk_analysis(enhanced_results)
    
    # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –ë–î
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    risk_level_filter = request.args.get('riskLevel', 'all')
    date_range = int(request.args.get('dateRange', 30))  # –¥–Ω–µ–π
    analysis_type = request.args.get('analysisType', 'all')
    
    if latest_db_path and os.path.exists(latest_db_path):
        from aml_database_setup import AMLDatabaseManager
        try:
            with AMLDatabaseManager(db_path=latest_db_path) as db:
                cursor = db.connection.cursor()
                
                # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –¥–∞—Ç–µ
                date_filter = ""
                if date_range > 0:
                    start_date = (datetime.now() - timedelta(days=date_range)).strftime('%Y-%m-%d')
                    date_filter = f"WHERE transaction_date >= '{start_date}'"
                
                # –ü–æ–¥—Å—á–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –ø–æ —É—Ä–æ–≤–Ω—è–º —Ä–∏—Å–∫–∞ —Å —É—á–µ—Ç–æ–º –¥–∞—Ç—ã
                cursor.execute(f'''
                SELECT 
                    COUNT(CASE WHEN final_risk_score > 3.0 OR is_suspicious = 1 THEN 1 END) as high_risk,
                    COUNT(CASE WHEN final_risk_score > 1.5 AND final_risk_score <= 3.0 AND is_suspicious = 0 THEN 1 END) as medium_risk,
                    COUNT(CASE WHEN final_risk_score <= 1.5 AND is_suspicious = 0 THEN 1 END) as low_risk,
                    COUNT(*) as total
                FROM transactions
                {date_filter}
                ''')
                
                risk_stats = dict(cursor.fetchone())
                
                # –§–∏–ª—å—Ç—Ä –¥–ª—è –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
                where_conditions = []
                if date_filter:
                    where_conditions.append(date_filter.replace("WHERE ", ""))
                    
                # –§–∏–ª—å—Ç—Ä –ø–æ —É—Ä–æ–≤–Ω—é —Ä–∏—Å–∫–∞
                if risk_level_filter == 'high':
                    where_conditions.append("(final_risk_score > 3.0 OR is_suspicious = 1)")
                elif risk_level_filter == 'medium':
                    where_conditions.append("(final_risk_score > 1.5 AND final_risk_score <= 3.0 AND is_suspicious = 0)")
                elif risk_level_filter == 'low':
                    where_conditions.append("(final_risk_score <= 1.5 AND is_suspicious = 0)")
                else:  # all - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
                    pass  # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ —Ä–∏—Å–∫—É –¥–ª—è "all"
                
                where_clause = "WHERE " + " AND ".join(where_conditions) if where_conditions else ""
                
                # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –∞–Ω–∞–ª–∏–∑–∞
                if analysis_type != 'all':
                    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                    cursor.execute(f'''
                    SELECT 
                        transaction_id,
                        sender_name,
                        beneficiary_name,
                        amount_kzt,
                        transaction_date,
                        final_risk_score,
                        risk_indicators,
                        rule_triggers,
                        suspicious_reasons
                    FROM transactions
                    {where_clause}
                    ORDER BY final_risk_score DESC
                    ''')
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ç–∏–ø—É –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
                    filtered_transactions = []
                    for row in cursor.fetchall():
                        tx = dict(row)
                        rule_triggers = tx.get('rule_triggers')
                        
                        if rule_triggers and isinstance(rule_triggers, str):
                            try:
                                rules = json.loads(rule_triggers)
                                if isinstance(rules, list):
                                    should_include = False
                                    
                                    for rule in rules:
                                        rule_lower = rule.lower()
                                        
                                        if analysis_type == 'transactional' and any(keyword in rule_lower for keyword in ['–∫—Ä—É–≥–ª–∞—è', '—Å—É–º–º–∞', '–≤—Ä–µ–º—è', '–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ']):
                                            should_include = True
                                            break
                                        elif analysis_type == 'network' and any(keyword in rule for keyword in ['–°–ï–¢–¨', '—Å—Ö–µ–º–∞', '–¥—Ä–æ–±–ª–µ–Ω–∏–µ']):
                                            should_include = True
                                            break
                                        elif analysis_type == 'behavioral' and any(keyword in rule for keyword in ['–ü–û–í–ï–î–ï–ù–ò–ï', '–≥–µ–æ–≥—Ä–∞—Ñ–∏—è']):
                                            should_include = True
                                            break
                                        elif analysis_type == 'customer' and '–∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç' in rule_lower:
                                            should_include = True
                                            break
                                        elif analysis_type == 'geographic' and any(keyword in rule_lower for keyword in ['—Å—Ç—Ä–∞–Ω–∞', '—é—Ä–∏—Å–¥–∏–∫—Ü–∏—è']):
                                            should_include = True
                                            break
                                    
                                    if should_include:
                                        # –ü–∞—Ä—Å–∏–º JSON –ø–æ–ª—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                                        if tx.get('risk_indicators') and isinstance(tx['risk_indicators'], str):
                                            try:
                                                tx['risk_indicators'] = json.loads(tx['risk_indicators'])
                                            except:
                                                pass
                                        if tx.get('rule_triggers') and isinstance(tx['rule_triggers'], str):
                                            try:
                                                tx['rule_triggers'] = json.loads(tx['rule_triggers'])
                                            except:
                                                pass
                                        filtered_transactions.append(tx)
                            except:
                                pass
                    
                    suspicious_transactions = filtered_transactions[:100]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 100
                else:
                    # –ï—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä –Ω–µ –ø—Ä–∏–º–µ–Ω–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å
                    cursor.execute(f'''
                    SELECT 
                        transaction_id,
                        sender_name,
                        beneficiary_name,
                        amount_kzt,
                        transaction_date,
                        final_risk_score,
                        risk_indicators,
                        rule_triggers,
                        suspicious_reasons
                    FROM transactions
                    {where_clause}
                    ORDER BY final_risk_score DESC
                    LIMIT 100
                    ''')
                    
                    suspicious_transactions = []
                    for row in cursor.fetchall():
                        tx = dict(row)
                        # –ü–∞—Ä—Å–∏–º JSON –ø–æ–ª—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                        if tx.get('risk_indicators') and isinstance(tx['risk_indicators'], str):
                            try:
                                tx['risk_indicators'] = json.loads(tx['risk_indicators'])
                            except:
                                pass
                        if tx.get('rule_triggers') and isinstance(tx['rule_triggers'], str):
                            try:
                                tx['rule_triggers'] = json.loads(tx['rule_triggers'])
                            except:
                                pass
                        suspicious_transactions.append(tx)
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ —Ä–∏—Å–∫–∞ —Å —É—á–µ—Ç–æ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤
                risk_indicators_count = {}
                cursor.execute(f'SELECT risk_indicators, rule_triggers FROM transactions {where_clause}')
                
                # –°—á–µ—Ç—á–∏–∫–∏ –ø–æ —Ç–∏–ø–∞–º –∞–Ω–∞–ª–∏–∑–∞
                analysis_type_counts = {
                    'transactional': 0,
                    'customer': 0,
                    'network': 0,
                    'behavioral': 0,
                    'geographic': 0
                }
                
                for row in cursor.fetchall():
                    # –ü–æ–¥—Å—á–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
                    indicators = row[0] if len(row) > 0 else None
                    if isinstance(indicators, str):
                        try:
                            indicators = json.loads(indicators)
                            if isinstance(indicators, dict):
                                for key, value in indicators.items():
                                    if value:
                                        risk_indicators_count[key] = risk_indicators_count.get(key, 0) + 1
                        except:
                            pass
                    
                    # –ü–æ–¥—Å—á–µ—Ç –ø–æ —Ç–∏–ø–∞–º –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ rule_triggers
                    rule_triggers = row[1] if len(row) > 1 else None
                    if rule_triggers and isinstance(rule_triggers, str):
                        try:
                            rules = json.loads(rule_triggers)
                            if isinstance(rules, list):
                                has_transactional = False
                                has_network = False
                                has_behavioral = False
                                has_customer = False
                                has_geographic = False
                                
                                for rule in rules:
                                    rule_lower = rule.lower()
                                    # –¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                                    if any(keyword in rule_lower for keyword in ['–∫—Ä—É–≥–ª–∞—è', '—Å—É–º–º–∞', '–≤—Ä–µ–º—è', '–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ']):
                                        has_transactional = True
                                    # –°–µ—Ç–µ–≤–æ–π –∞–Ω–∞–ª–∏–∑
                                    elif any(keyword in rule_lower for keyword in ['—Å–µ—Ç—å', '—Å—Ö–µ–º–∞', '–¥—Ä–æ–±–ª–µ–Ω–∏–µ']):
                                        has_network = True
                                    # –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
                                    elif any(keyword in rule_lower for keyword in ['–ø–æ–≤–µ–¥–µ–Ω–∏–µ', '–≥–µ–æ–≥—Ä–∞—Ñ–∏—è']):
                                        has_behavioral = True
                                    # –ö–ª–∏–µ–Ω—Ç—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
                                    elif '–∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç' in rule_lower:
                                        has_customer = True
                                    # –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
                                    elif any(keyword in rule_lower for keyword in ['—Å—Ç—Ä–∞–Ω–∞', '—é—Ä–∏—Å–¥–∏–∫—Ü–∏—è']):
                                        has_geographic = True
                                
                                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫–∏
                                if has_transactional:
                                    analysis_type_counts['transactional'] += 1
                                if has_network:
                                    analysis_type_counts['network'] += 1
                                if has_behavioral:
                                    analysis_type_counts['behavioral'] += 1
                                if has_customer:
                                    analysis_type_counts['customer'] += 1
                                if has_geographic:
                                    analysis_type_counts['geographic'] += 1
                        except:
                            pass
                
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –ø–æ —á–∞—Å—Ç–æ—Ç–µ
                top_indicators = sorted(risk_indicators_count.items(), key=lambda x: x[1], reverse=True)[:10]
                
                return jsonify({
                    'risk_summary': {
                        'high': risk_stats['high_risk'] or 0,
                        'medium': risk_stats['medium_risk'] or 0,
                        'low': risk_stats['low_risk'] or 0,
                        'total': risk_stats['total'] or 0
                    },
                    'suspicious_transactions': suspicious_transactions,
                    'top_risk_indicators': [{'name': name, 'count': count} for name, count in top_indicators],
                    'analysis_type_breakdown': analysis_type_counts,
                    'filters_applied': {
                        'risk_level': risk_level_filter,
                        'date_range': date_range,
                        'analysis_type': analysis_type
                    },
                    'last_updated': datetime.now().isoformat()
                })
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ —Ä–∏—Å–∫–æ–≤: {e}")  
            return jsonify({
                'risk_summary': {
                    'high': 0,
                    'medium': 0,
                    'low': 0,
                    'total': 0
                },
                'suspicious_transactions': [],
                'top_risk_indicators': [],
                'analysis_type_breakdown': {
                    'transactional': 0,
                    'customer': 0,
                    'network': 0,
                    'behavioral': 0,
                    'geographic': 0
                },
                'filters_applied': {
                    'risk_level': risk_level_filter,
                    'date_range': date_range,
                    'analysis_type': analysis_type
                },
                'last_updated': datetime.now().isoformat(),
                'error': str(e)
            })
    
    # –ï—Å–ª–∏ –ë–î –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
    return jsonify({
        'risk_summary': {
            'high': 0,
            'medium': 0,
            'low': 0,
            'total': 0
        },
        'suspicious_transactions': [],
        'top_risk_indicators': [],
        'analysis_type_breakdown': {
            'transactional': 0,
            'customer': 0,
            'network': 0,
            'behavioral': 0,
            'geographic': 0
        },
        'filters_applied': {
            'risk_level': risk_level_filter,
            'date_range': date_range,
            'analysis_type': analysis_type
        },
        'last_updated': datetime.now().isoformat()
    })

# –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ (–±–µ–∑ API –ø—Ä–µ—Ñ–∏–∫—Å–∞)
@app.route('/')
def index():
    return jsonify({
        'service': '–ê–§–ú –†–ö - –°–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π',
        'api_docs': '/api/',
        'health_check': '/api/health'
    })

@api_bp.route('/analytics/run-enhanced', methods=['POST'])
def run_enhanced_analysis_endpoint():
    """–ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ—Ñ–∏–ª—è–º–∏"""
    try:
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –∞–Ω–∞–ª–∏–∑–∞
        task_id = f"enhanced_{int(datetime.now().timestamp())}"
        processing_tasks[task_id] = {
            'status': 'starting',
            'message': '–ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...',
            'progress': 0,
            'start_time': datetime.now().isoformat()
        }
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        def run_analysis():
            try:
                processing_tasks[task_id]['status'] = 'processing'
                processing_tasks[task_id]['message'] = '–ê–Ω–∞–ª–∏–∑ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ—Ñ–∏–ª—è–º–∏...'
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
                results_path = run_enhanced_analysis_on_existing_data()
                
                processing_tasks[task_id]['status'] = 'completed'
                processing_tasks[task_id]['message'] = '–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!'
                processing_tasks[task_id]['progress'] = 100
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                with open(results_path, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                
                processing_tasks[task_id]['results'] = results
                
            except Exception as e:
                processing_tasks[task_id]['status'] = 'failed'
                processing_tasks[task_id]['message'] = f'–û—à–∏–±–∫–∞: {str(e)}'
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —É–ª—É—á—à–µ–Ω–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ: {e}")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        thread = threading.Thread(target=run_analysis)
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'message': '–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–ø—É—â–µ–Ω',
            'status_url': f'/api/processing/{task_id}'
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

def run_enhanced_analysis_on_existing_data() -> str:
    """–ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î"""
    try:
        print(f"üîÑ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {latest_db_path}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è pipeline  
        pipeline = UnifiedAMLPipeline()
        pipeline._initialize_database(latest_db_path)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
        with AMLDatabaseManager(latest_db_path) as db:
            cursor = db.connection.cursor()
            cursor.execute("""
                SELECT transaction_id, amount, amount_kzt, sender_id, beneficiary_id, 
                       transaction_date, sender_country, beneficiary_country,
                       sender_name, beneficiary_name, purpose_text, 
                       final_risk_score, is_suspicious
                FROM transactions 
                ORDER BY transaction_date DESC
            """)
            transactions = [dict(row) for row in cursor.fetchall()]
        
        print(f"üìä –ê–Ω–∞–ª–∏–∑ {len(transactions)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π...")
        
        results = []
        total_transactions = len(transactions)
        
        for i, tx in enumerate(transactions):
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 1000 —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
            if i % 1000 == 0:
                print(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{total_transactions} ({i/total_transactions*100:.1f}%)")
            try:
                transaction = {
                    'transaction_id': tx['transaction_id'],
                    'amount': float(tx['amount']),
                    'amount_kzt': float(tx.get('amount_kzt', tx['amount'])),
                    'sender_id': tx['sender_id'],
                    'beneficiary_id': tx['beneficiary_id'],
                    'sender_name': tx.get('sender_name', ''),
                    'beneficiary_name': tx.get('beneficiary_name', ''),
                    'purpose_text': tx.get('purpose_text', ''),
                    'transaction_date': tx['transaction_date'],
                    'date': datetime.strptime(tx['transaction_date'], '%Y-%m-%d %H:%M:%S'),
                    'sender_country': tx.get('sender_country', 'KZ'),
                    'beneficiary_country': tx.get('beneficiary_country', 'KZ'),
                    'final_risk_score': float(tx.get('final_risk_score', 0)),
                    'is_suspicious': bool(tx.get('is_suspicious', False))
                }
                
                result = pipeline._analyze_single_transaction(transaction)
                
                results.append({
                    'transaction_id': tx['transaction_id'],
                    'overall_risk': result.overall_risk,
                    'risk_category': result.risk_category,
                    'transaction_risk': result.transaction_risk,
                    'customer_risk': result.customer_risk,
                    'network_risk': result.network_risk,
                    'behavioral_risk': result.behavioral_risk,
                    'geographic_risk': result.geographic_risk,
                    'suspicious_flags': result.suspicious_flags,
                    'explanations': result.explanations
                })
                
                if (i + 1) % 50 == 0:
                    print(f"  ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{len(transactions)} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")
                    
            except Exception as e:
                print(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")
                continue
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results_file = os.path.join(RESULTS_FOLDER, f'enhanced_existing_{int(datetime.now().timestamp())}.json')
        
        summary = {
            'timestamp': datetime.now().isoformat(),
            'total_transactions': len(transactions),
            'analyzed_transactions': len(results),
            'high_risk_count': len([r for r in results if r['overall_risk'] >= 4.0]),
            'medium_risk_count': len([r for r in results if 2.0 <= r['overall_risk'] < 4.0]),
            'low_risk_count': len([r for r in results if r['overall_risk'] < 2.0]),
            'average_risk': sum(r['overall_risk'] for r in results) / len(results) if results else 0,
            'pipeline_version': 'unified_enhanced_existing',
            'db_path': latest_db_path,
            'results': results
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω! –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫: {summary['average_risk']:.2f}")
        
        return results_file
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise

# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º Blueprint
app.register_blueprint(api_bp)

# –ü—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –ë–î
def find_latest_db():
    """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–æ–∑–¥–∞–Ω–Ω—É—é –ë–î –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    global latest_db_path
    db_pattern = os.path.join(os.path.dirname(__file__), "aml_system_*.db")
    db_files = glob.glob(db_pattern)
    if db_files:
        # –ë–µ—Ä–µ–º —Å–∞–º—É—é –Ω–æ–≤—É—é –ë–î –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
        latest_db_path = max(db_files, key=os.path.getmtime)
        print(f"üîÑ –ù–∞–π–¥–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –ë–î: {os.path.basename(latest_db_path)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –Ω–µ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
        try:
            from aml_database_setup import AMLDatabaseManager
            db = AMLDatabaseManager(db_path=latest_db_path)
            cursor = db.connection.cursor()
            cursor.execute("SELECT COUNT(*) FROM transactions")
            count = cursor.fetchone()[0]
            print(f"üìä –í –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö {count} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")
            
            # –ï—Å–ª–∏ –≤ —Ç–µ–∫—É—â–µ–π –ë–î –Ω–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π, –∏—â–µ–º –ë–î —Å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏
            if count == 0:
                for db_file in sorted(db_files, key=os.path.getmtime, reverse=True):
                    temp_db = AMLDatabaseManager(db_path=db_file)
                    cursor = temp_db.connection.cursor()
                    cursor.execute("SELECT COUNT(*) FROM transactions")
                    trans_count = cursor.fetchone()[0]
                    temp_db.close()
                    
                    if trans_count > 0:
                        latest_db_path = db_file
                        print(f"‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ –ë–î —Å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏: {os.path.basename(db_file)} ({trans_count} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π)")
                        break
            
            db.close()
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ë–î: {e}")

# –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ë–î –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
find_latest_db()

# –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ: —è–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö —Å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏
target_db = "aml_system_e840b2937714940f.db"
if os.path.exists(os.path.join(os.path.dirname(__file__), target_db)):
    latest_db_path = os.path.join(os.path.dirname(__file__), target_db)
    print(f"üéØ –Ø–≤–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ë–î: {os.path.basename(latest_db_path)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
    try:
        from aml_database_setup import AMLDatabaseManager
        db = AMLDatabaseManager(db_path=latest_db_path)
        cursor = db.connection.cursor()
        cursor.execute("SELECT COUNT(*) FROM transactions")
        count = cursor.fetchone()[0]
        print(f"üìä –í —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–π –ë–î {count} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")
        db.close()
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ë–î: {e}")
else:
    print(f"‚ùå –¶–µ–ª–µ–≤–∞—è –ë–î {target_db} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")

if __name__ == '__main__':
    print("=" * 50)
    print("–ê–§–ú –†–ö - –ë—ç–∫–µ–Ω–¥ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π")
    print("=" * 50)
    print("–°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ http://localhost:8000")
    print("API –¥–æ—Å—Ç—É–ø–Ω–æ –Ω–∞ http://localhost:8000/api/")
    print("–§—Ä–æ–Ω—Ç–µ–Ω–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∑–∞–ø—É—â–µ–Ω –Ω–∞ http://localhost:3000")
    print("=" * 50)
    
    app.run(host='0.0.0.0', port=8000, debug=True)