#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤: –∫–ª–∏–µ–Ω—Ç—Å–∫–∏–π, —Å–µ—Ç–µ–≤–æ–π, –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–π, –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π
"""

import sys
import json
from datetime import datetime
from unified_aml_pipeline import UnifiedAMLPipeline
from aml_database_setup import AMLDatabaseManager

def test_all_analyzers():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≤—Å–µ —Ç–∏–ø—ã –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–π –≤—ã–±–æ—Ä–∫–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π"""
    
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è pipeline
    pipeline = UnifiedAMLPipeline()
    pipeline._initialize_database('aml_system_e840b2937714940f.db')
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    with AMLDatabaseManager('aml_system_e840b2937714940f.db') as db:
        cursor = db.connection.cursor()
        cursor.execute("""
            SELECT transaction_id, amount, amount_kzt, sender_id, beneficiary_id, 
                   transaction_date, sender_country, beneficiary_country,
                   sender_name, beneficiary_name, purpose_text, 
                   final_risk_score, is_suspicious
            FROM transactions 
            ORDER BY transaction_date DESC
            LIMIT 100
        """)
        transactions = [dict(zip([desc[0] for desc in cursor.description], row)) 
                       for row in cursor.fetchall()]
    
    print(f"üìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {len(transactions)} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π...")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
    results = []
    analysis_counts = {
        'transaction': 0,
        'customer': 0, 
        'network': 0,
        'behavioral': 0,
        'geographic': 0,
        'total_high_risk': 0,
        'total_medium_risk': 0,
        'total_low_risk': 0
    }
    
    for i, tx in enumerate(transactions):
        result = pipeline._analyze_single_transaction(tx)
        results.append(result)
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –∞–Ω–∞–ª–∏–∑—ã (—Ä–∏—Å–∫ > 0)
        if result.transaction_risk > 0:
            analysis_counts['transaction'] += 1
        if result.customer_risk > 0:
            analysis_counts['customer'] += 1
        if result.network_risk > 0:
            analysis_counts['network'] += 1
        if result.behavioral_risk > 0:
            analysis_counts['behavioral'] += 1
        if result.geographic_risk > 0:
            analysis_counts['geographic'] += 1
            
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ä–∏—Å–∫–∞
        if result.overall_risk >= 4.0:
            analysis_counts['total_high_risk'] += 1
        elif result.overall_risk >= 2.0:
            analysis_counts['total_medium_risk'] += 1
        else:
            analysis_counts['total_low_risk'] += 1
        
        if (i + 1) % 20 == 0:
            print(f"üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {i + 1}/{len(transactions)}")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    report = {
        'timestamp': datetime.now().isoformat(),
        'total_analyzed': len(transactions),
        'analysis_activation': analysis_counts,
        'sample_results': [
            {
                'transaction_id': r.client_id,
                'transaction_risk': r.transaction_risk,
                'customer_risk': r.customer_risk,
                'network_risk': r.network_risk,
                'behavioral_risk': r.behavioral_risk,
                'geographic_risk': r.geographic_risk,
                'overall_risk': r.overall_risk,
                'risk_category': r.risk_category,
                'flags_count': len(r.suspicious_flags),
                'explanations_count': len(r.explanations)
            } for r in results[:10]  # –ü–µ—Ä–≤—ã–µ 10 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
        ]
    }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    report_file = f"analyzer_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –ê–ù–ê–õ–ò–ó–ê–¢–û–†–û–í:")
    print("=" * 50)
    print(f"üìà –í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {report['total_analyzed']}")
    print("\nüîç –ê–∫—Ç–∏–≤–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤:")
    for analyzer, count in analysis_counts.items():
        if analyzer.startswith('total_'):
            continue
        percentage = (count / len(transactions)) * 100
        print(f"  {analyzer.capitalize()}: {count}/{len(transactions)} ({percentage:.1f}%)")
    
    print(f"\nüéØ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–æ–≤:")
    print(f"  –í—ã—Å–æ–∫–∏–π (‚â•4.0): {analysis_counts['total_high_risk']}")
    print(f"  –°—Ä–µ–¥–Ω–∏–π (2.0-4.0): {analysis_counts['total_medium_risk']}")
    print(f"  –ù–∏–∑–∫–∏–π (<2.0): {analysis_counts['total_low_risk']}")
    
    print(f"\nüíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
    
    return report

if __name__ == "__main__":
    test_all_analyzers()